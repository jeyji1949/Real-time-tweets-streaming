PARTIE PERSONNE 2 â€” ANALYSE & INDEXATION DES TWEETS (DÃ‰TAIL COMPLET)

ðŸŸ¦ PARTIE 1 â€” Consommation des tweets depuis Kafka
ðŸŽ¯ Objectif
Lire les tweets produits par le simulateur (Personne 1) depuis Kafka en temps rÃ©el.

ðŸ”§ Code (extrait analyzer.py)
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    "tweets_topic",
    bootstrap_servers="kafka:29092",
    value_deserializer=lambda x: json.loads(x.decode("utf-8")),
    auto_offset_reset="earliest",
    enable_auto_commit=True
)

âŒ¨ï¸ Commandes utilisÃ©es
docker compose up -d --build
docker logs -f analyzer

ðŸ“Š RÃ©sultat obtenu
ðŸ”¥ ANALYZER BOOTING...
âœ… Kafka connected
ï‘‚ Waiting for Kafka messages...
ðŸ‘‰ Kafka fonctionne et les tweets sont bien reÃ§us

ðŸŸ¦ PARTIE 2 â€” Analyse de sentiment avec TextBlob
ðŸŽ¯ Objectif
DÃ©terminer si un tweet est positif, nÃ©gatif ou neutre.

ðŸ”§ Code (analyzer.py)
from textblob import TextBlob

def analyze_with_textblob(text):
    polarity = TextBlob(text).sentiment.polarity
    
    if polarity > 0.1:
        return "positive", 1
    elif polarity < -0.1:
        return "negative", -1
    else:
        return "neutral", 0

ðŸ“Š RÃ©sultat obtenu
ðŸ’¬ Sentiment: positive | â­ Score: 1
ðŸ’¬ Sentiment: neutral  | â­ Score: 0
ðŸ‘‰ Score normalisÃ© :
    â€¢ 1 â†’ positif
    â€¢ 0 â†’ neutre
    â€¢ -1 â†’ nÃ©gatif

â— ProblÃ¨me rencontrÃ©
Au dÃ©but :
    â€¢ uniquement 0 et 1
    â€¢ jamais -1
âœ… Correction
Ajout dâ€™un seuil nÃ©gatif polarity < -0.1

ðŸŸ¦ PARTIE 3 â€” DÃ©tection de topic dynamique
ðŸŽ¯ Objectif
Associer chaque tweet Ã  un thÃ¨me logique.

ðŸ”§ Code (topic mapping)
TOPIC_KEYWORDS = {
    "AI": ["ai", "machine", "learning", "neural"],
    "Cloud": ["cloud", "aws", "azure"],
    "Security": ["cyber", "security", "infosec"],
    "Web": ["javascript", "web", "frontend"],
    "Data": ["data", "pipeline", "bigdata"]
}

def detect_topic(text):
    text = text.lower()
    for topic, keywords in TOPIC_KEYWORDS.items():
        if any(k in text for k in keywords):
            return topic
    return "General Tech"

ðŸ“Š RÃ©sultat obtenu
ðŸ“Œ Topic: AI
ðŸ“Œ Topic: Security
ðŸ“Œ Topic: Data
ðŸ‘‰ ProblÃ¨me initial :
Tous les tweets retournaient technology
ðŸ‘‰ Solution :
Topic dynamique par mots-clÃ©s

ðŸŸ¦ PARTIE 4 â€” Extraction des hashtags & frÃ©quence des mots
ðŸŽ¯ Objectif
Permettre :
    â€¢ hashtags les plus utilisÃ©s
    â€¢ mots frÃ©quents
    â€¢ meilleurs / pires tweets

ðŸ”§ Code (extrait)
import re
from collections import Counter

def extract_hashtags(text):
    return re.findall(r"#(\w+)", text)

def word_frequency(text):
    words = re.findall(r"\b\w+\b", text.lower())
    return dict(Counter(words))

ðŸ“Š RÃ©sultat indexÃ©
"hashtags": ["Cybersecurity", "InfoSec"],
"word_freq": {
  "api": 1,
  "design": 1,
  "security": 1
}

ðŸŸ¦ PARTIE 5 â€” Indexation dans Elasticsearch
ðŸŽ¯ Objectif
Stocker les tweets enrichis pour analyse.

ðŸ”§ Mapping Elasticsearch
curl -X PUT http://localhost:9200/tweets_index \
-H "Content-Type: application/json" \
-d '{
  "mappings": {
    "properties": {
      "tweet_id": { "type": "keyword" },
      "text": { "type": "text" },
      "hashtags": { "type": "keyword" },
      "sentiment": { "type": "keyword" },
      "score": { "type": "integer" },
      "topic": { "type": "keyword" },
      "analysis_method": { "type": "keyword" }
    }
  }
}'

ðŸ”§ Code dâ€™indexation
es.index(index="tweets_index", document=tweet_doc)

ðŸ“Š VÃ©rification
curl http://localhost:9200/tweets_index/_search?size=1&pretty
RÃ©sultat :
{
  "text": "New breakthrough in CI/CD!",
  "sentiment": "positive",
  "score": 1,
  "topic": "Security"
}

ðŸŸ¦ PARTIE 6 â€” Kibana (Visualisation)
ðŸŽ¯ Objectif
Analyser les donnÃ©es sans coder.

Ã‰tapes Kibana
    1. AccÃ©der Ã 
ðŸ‘‰ http://localhost:5601
    2. Create Data View
        â—¦ Index: tweets_index
        â—¦ Time field: created_at
    3. Visualisations :
        â—¦ Pie â†’ sentiment
        â—¦ Bar â†’ topic
        â—¦ Tag cloud â†’ hashtags
    4. Dashboard :
Real-Time Tweets Analytics

ðŸŸ¦ PARTIE 7 â€” Dockerisation
ðŸŽ¯ Objectif
Pipeline automatisÃ© et reproductible.

ðŸ”§ Dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY analyzer.py .
CMD ["python", "analyzer.py"]

ðŸ”§ Commandes Docker
docker compose down
docker compose up -d --build
docker ps

ðŸŸ¦ PARTIE 8 â€” ProblÃ¨mes rencontrÃ©s & solutions
âŒ ProblÃ¨mes
    â€¢ OpenAI API quota (429)
    â€¢ Ollama mÃ©moire insuffisante
    â€¢ Kafka module manquant en local
    â€¢ Mapping Elasticsearch Ã©crasÃ©
âœ… Solutions
    â€¢ Abandon OpenAI & Ollama
    â€¢ TextBlob stable & gratuit
    â€¢ Kafka uniquement via Docker
    â€¢ Mapping fixÃ© une seule fois

## Docker Integration

The analyzer service was added to the root docker-compose.yml to integrate
the sentiment analysis pipeline with Kafka and Elasticsearch.
No existing services were modified.

ðŸŸ¦ PARTIE 9 â€” Handoff Personne 3
ðŸŽ LivrÃ© Ã  Personne 3
    â€¢ Index tweets_index
    â€¢ Mapping validÃ©
    â€¢ Dashboard Kibana
    â€¢ Champs prÃªts pour statistiques :
        â—¦ hashtags
        â—¦ sentiment
        â—¦ score
        â—¦ topic
        â—¦ word_freq

