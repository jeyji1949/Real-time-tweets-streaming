# ğŸ¦ Twitter Real-Time Analysis Pipeline

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Kafka](https://img.shields.io/badge/Kafka-3.6-red.svg)](https://kafka.apache.org)
[![Docker](https://img.shields.io/badge/Docker-required-blue.svg)](https://docker.com)

SystÃ¨me d'analyse de tweets en temps rÃ©el utilisant Kafka, OpenAI, Elasticsearch, Cassandra et Kibana.

## ğŸ¯ Objectifs du projet

Analyser des tweets en temps rÃ©el pour extraire :
- âœ… Les hashtags les plus utilisÃ©s
- âœ… Les statistiques des sentiments (positif/nÃ©gatif/neutre)
- âœ… Les mots les plus frÃ©quents
- âœ… Les meilleurs et pires tweets
- âœ… Visualisations interactives avec Kibana

---

## ğŸ—ï¸ Architecture
```
[Twitter API] 
    â†“
[Kafka Producer] â†’ [Topic: tweets_raw]
    â†“
[Kafka Consumer]
    â†“
[OpenAI API] (analyse sentiment + topics)
    â†“
[Elasticsearch] (indexation + recherche)
    â†“
[Cassandra] (stockage permanent)
    â†“
[Kibana] (visualisation + dashboards)
```

**SchÃ©ma dÃ©taillÃ© :** Voir [docs/architecture.md](docs/architecture.md)

---

## ğŸ‘¥ Ã‰quipe & ResponsabilitÃ©s

| Membre | OS | Composants | Dossiers |
|--------|----|-----------|-----------------------|
| **Personne 1** | Linux | Kafka + Twitter Stream | `producer/`, `consumer/`, `data/` |
| **Personne 2** | Linux | OpenAI + Elasticsearch | `analysis/` |
| **Personne 3** | Windows | Cassandra + Kibana | `storage/`, `dashboards/` |

---

## ğŸš€ Installation & Setup

### PrÃ©requis

- **Python 3.8+**
- **Docker & Docker Compose**
- **Compte dÃ©veloppeur Twitter** ([developer.twitter.com](https://developer.twitter.com))
- **ClÃ© API OpenAI** (pour Personne 2)

### Setup rapide
```bash
# 1. Cloner le repo
git clone https://github.com/votre-username/Twitter-Project.git
cd Twitter-Project

# 2. CrÃ©er l'environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. Configurer les variables d'environnement
cp .env.example .env
nano .env  # Ajouter vos clÃ©s API

# 5. Lancer Docker (Kafka, Elasticsearch, Kibana, Cassandra)
docker-compose up -d

# 6. VÃ©rifier que tous les services sont UP
docker-compose ps
```

---

## ğŸ® Utilisation

### Terminal 1 : Lancer le Producer (Twitter â†’ Kafka)
```bash
source venv/bin/activate
cd producer/
python twitter_stream_producer.py
```

### Terminal 2 : Lancer le Consumer (Kafka â†’ Analyse)
```bash
source venv/bin/activate
cd consumer/
python consumer.py
```

### Terminal 3 : Analyse OpenAI (Personne 2)
```bash
source venv/bin/activate
cd analysis/
python openai_analyzer.py
```

---

## ğŸ“Š AccÃ¨s aux interfaces

| Service | URL | Description |
|---------|-----|-------------|
| **Kibana** | http://localhost:5601 | Dashboards & visualisations |
| **Elasticsearch** | http://localhost:9200 | API REST pour requÃªtes |
| **Kafka** | localhost:9092 | Broker Kafka |
| **Cassandra** | localhost:9042 | Base de donnÃ©es NoSQL |

---

## ğŸ“ Structure du projet
```
Twitter-Project/
â”œâ”€â”€ producer/          # Stream Twitter â†’ Kafka
â”œâ”€â”€ consumer/          # Kafka â†’ Processing
â”œâ”€â”€ analysis/          # OpenAI + Elasticsearch (Personne 2)
â”œâ”€â”€ storage/           # Cassandra setup (Personne 3)
â”œâ”€â”€ dashboards/        # Kibana dashboards (Personne 3)
â”œâ”€â”€ data/              # Datasets & samples
â”œâ”€â”€ docs/              # Documentation technique
â””â”€â”€ tests/             # Tests unitaires
```

**Voir les README spÃ©cifiques dans chaque dossier pour plus de dÃ©tails.**

---

## ğŸ”§ Configuration

### Variables d'environnement (.env)
```bash
# Twitter API
TWITTER_BEARER_TOKEN=your_token_here
TWITTER_API_KEY=your_key_here
TWITTER_API_SECRET=your_secret_here

# Kafka
KAFKA_BROKER=localhost:9092
KAFKA_TOPIC_RAW=tweets_raw
KAFKA_TOPIC_ANALYZED=tweets_analyzed

# OpenAI (Personne 2)
OPENAI_API_KEY=your_openai_key_here

# Elasticsearch
ELASTICSEARCH_HOST=localhost:9200

# Cassandra
CASSANDRA_HOST=localhost:9042
```

**âš ï¸ Ne jamais commit le fichier `.env` ! Utilisez `.env.example` comme template.**

---

## ğŸ“š Documentation

- [Architecture dÃ©taillÃ©e](docs/04-architecture.md)
- [SchÃ©ma JSON](docs/schema.json)
- [Guide de setup complet](docs/01-setup-guide.md)
- [Producer README](producer/README.md)
- [Consumer README](consumer/README.md)
- [Analysis README](analysis/README.md)

---

## ğŸ§ª Tests
```bash
# Tester la connexion Twitter
python producer/test_twitter.py

# Tester Kafka
python tests/test_kafka.py

# Tester Elasticsearch
python tests/test_elasticsearch.py
```

---

## ğŸ› DÃ©pannage

### Kafka ne dÃ©marre pas
```bash
docker-compose down
docker-compose up -d
docker logs kafka
```

### Tweets n'arrivent pas
- VÃ©rifier le Bearer Token dans `.env`
- VÃ©rifier les rÃ¨gles de filtrage dans `producer/twitter_stream_producer.py`
- Consulter les logs : `docker logs kafka`

### Elasticsearch inaccessible
```bash
curl http://localhost:9200
# Si erreur, restart: docker-compose restart elasticsearch
```

---

## ğŸ“ TODO & AmÃ©liorations

- [ ] Ajouter des tests unitaires
- [ ] ImplÃ©menter le retry logic pour OpenAI
- [ ] CrÃ©er des dashboards Kibana avancÃ©s
- [ ] Ajouter monitoring avec Prometheus
- [ ] Documentation API complÃ¨te

---

## ğŸ‘¨â€ğŸ’» Contributeurs

- **Personne 1** - Kafka Pipeline & Twitter Integration
- **Personne 2** - OpenAI Analysis & Elasticsearch
- **Personne 3** - Cassandra & Kibana Dashboards

---
## âœ… Ã‰tat d'avancement - Personne 1 (Kafka Pipeline)

### Infrastructure âœ… TERMINÃ‰
- [x] Docker Compose configurÃ© avec 5 services
- [x] Kafka + Zookeeper opÃ©rationnels
- [x] Topic `tweets_raw` crÃ©Ã© automatiquement
- [x] Configuration rÃ©seau corrigÃ©e (ADVERTISED_LISTENERS)

### Code âœ… TERMINÃ‰
- [x] Simulateur de tweets rÃ©aliste (`producer/twitter_simulator.py`)
- [x] Consumer Kafka fonctionnel (`consumer/consumer.py`)
- [x] Tests de validation (`test_simple_producer.py`)
- [x] Scripts de dÃ©marrage automatique

### Documentation âœ… TERMINÃ‰
- [x] Guide d'installation complet (`docs/01-setup-guide.md`)
- [x] Guide de dÃ©monstration (`docs/02-demo.md`)
- [x] Guide de dÃ©pannage (`docs/03-troubleshooting.md`)
- [x] Architecture du systÃ¨me (`docs/04-architecture.md`)
- [x] SchÃ©ma JSON standardisÃ© (`docs/schema.json`)

### Pipeline âœ… OPÃ‰RATIONNEL
```
Simulator â†’ Kafka (tweets_raw) â†’ Consumer
  (1-3s)       (<100ms)            (real-time)
```

---

## ğŸ“Š DÃ©monstration rapide

### Lancer le pipeline

**Terminal 1 - Consumer :**
```bash
source venv/bin/activate
cd consumer && python consumer.py
```

**Terminal 2 - Producer :**
```bash
source venv/bin/activate
cd producer && python twitter_simulator.py
```

### RÃ©sultat attendu

Les tweets gÃ©nÃ©rÃ©s par le producer apparaissent instantanÃ©ment dans le consumer ! ğŸ‰

---

## ğŸ¤ Pour Personne 2 (OpenAI + Elasticsearch)

**Le pipeline Kafka est prÃªt !**

**Ce qui fonctionne :**
- âœ… Kafka sur `localhost:9092`
- âœ… Topic : `tweets_raw`
- âœ… Format : JSON (voir `docs/schema.json`)
- âœ… ~20 tweets/minute

**Pour dÃ©marrer :**
1. Lire le guide : `docs/01-setup-guide.md`
2. Voir le format : `docs/schema.json`
3. Se connecter Ã  Kafka :
```python
from kafka import KafkaConsumer
consumer = KafkaConsumer('tweets_raw', bootstrap_servers='localhost:9092')
```

**Prochaines Ã©tapes :**
1. Analyser chaque tweet avec OpenAI
2. Ajouter les champs : `sentiment`, `topic`, `confidence`
3. Indexer dans Elasticsearch

---

## ğŸ“š Documentation complÃ¨te

Toute la documentation se trouve dans `/docs` :
- **Setup** : [01-setup-guide.md](docs/01-setup-guide.md)
- **DÃ©mo** : [02-demo.md](docs/02-demo.md)
- **Troubleshooting** : [03-troubleshooting.md](docs/03-troubleshooting.md)
- **Architecture** : [04-architecture.md](docs/04-architecture.md)
- **Schema** : [schema.json](docs/schema.json)
```
---

## Collaboration rules
- Do NOT push to main
- Work only on your branch
- Use Pull Requests for merging

## ğŸ“„ Licence

Ce projet est Ã  usage Ã©ducatif dans le cadre du cours de Big Data.

---

## ğŸ†˜ Support

Pour toute question :
- Ouvrir une issue sur GitHub
- Contacter l'Ã©quipe par email
- Consulter la documentation dans `/docs`
